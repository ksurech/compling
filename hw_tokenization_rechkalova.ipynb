{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiXdJjBa9sCq"
      },
      "source": [
        "# Домашнее задание: Токенизация текста"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Дан список текстов, которые нужно токенизировать разными способами"
      ],
      "metadata": {
        "id": "1xVbvaj_phyN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = [\n",
        "\"The quick brown fox jumps over the lazy dog. It's a beautiful day!\",\n",
        "\"Dr. Smith arrived at 5:30 p.m. from New York. The meeting cost $1,000.50.\",\n",
        "\"I can't believe she's going! Let's meet at Jane's house. They'll love it.\",\n",
        "\"What's the ETA for the package? Please e-mail support@example.com ASAP!\"\n",
        "]"
      ],
      "metadata": {
        "id": "uj-xaNnwpiPo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Используйте способы токенизации, которые описаны в практикуме. Каждый способ нужно обернуть в функцию, например:\n",
        "\n",
        " ```python\n",
        " def simple_tokenization(string):\n",
        "   return string.split()\n",
        "   ```"
      ],
      "metadata": {
        "id": "ix1Im4Kcqb3_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Напишите функцию для токенизации по пробелам и знакам препинания (используйте оператор `def`)"
      ],
      "metadata": {
        "id": "Ih0BBOGBpv6Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Ваш код здесь\n",
        "\n",
        "import re\n",
        "\n",
        "def tokenize_by_space_punct(text):\n",
        "    \"\"\"\n",
        "    Токенизация текста по пробелам и знакам препинания.\n",
        "    Разделяет текст по пробелам и пунктуации, возвращает список токенов.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        tokens = re.split(r'[\\s,.!?;:\\\"()\\[\\]{}]+', text)\n",
        "        tokens = [t for t in tokens if t]  # удаление пустых элементов\n",
        "        return tokens\n",
        "    except Exception as e:\n",
        "        print(f\"Ошибка токенизации: {e}\")\n",
        "        return []"
      ],
      "metadata": {
        "id": "W1QCaw6cqDnn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Напишите функцию для токенизации текста с помощью NLTK"
      ],
      "metadata": {
        "id": "GThvPcovqgO5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ваш код здесь\n",
        "\n",
        "def tokenization_nltk(full_text):\n",
        "    from nltk.tokenize import word_tokenize\n",
        "    return word_tokenize(full_text)"
      ],
      "metadata": {
        "id": "14BIv33iqrkL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Напишите функцию для токенизации текста с помощью Spacy"
      ],
      "metadata": {
        "id": "GxW7ZP6iqwpt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ваш код здесь\n",
        "def tokenization_spacy(full_text):\n",
        "    import spacy\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "    doc = nlp(full_text)\n",
        "    return [token.text for token in doc]"
      ],
      "metadata": {
        "id": "B0NQg-VfuFW_"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. С помощью цикла `for` примените каждую из написанных функций к каждому тексту из списка `texts`"
      ],
      "metadata": {
        "id": "WmyJfB9wuKkm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "lvUmk94MhrL8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b02b4e83-71b0-4b04-def9-eb4d7c9880c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Текст 1: The quick brown fox jumps over the lazy dog. It's a beautiful day!\n",
            "NLTK токены: ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.', 'It', \"'s\", 'a', 'beautiful', 'day', '!']\n",
            "spaCy токены: ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.', 'It', \"'s\", 'a', 'beautiful', 'day', '!']\n",
            "--------------------------------------------------\n",
            "Текст 2: Dr. Smith arrived at 5:30 p.m. from New York. The meeting cost $1,000.50.\n",
            "NLTK токены: ['Dr.', 'Smith', 'arrived', 'at', '5:30', 'p.m.', 'from', 'New', 'York', '.', 'The', 'meeting', 'cost', '$', '1,000.50', '.']\n",
            "spaCy токены: ['Dr.', 'Smith', 'arrived', 'at', '5:30', 'p.m.', 'from', 'New', 'York', '.', 'The', 'meeting', 'cost', '$', '1,000.50', '.']\n",
            "--------------------------------------------------\n",
            "Текст 3: I can't believe she's going! Let's meet at Jane's house. They'll love it.\n",
            "NLTK токены: ['I', 'ca', \"n't\", 'believe', 'she', \"'s\", 'going', '!', 'Let', \"'s\", 'meet', 'at', 'Jane', \"'s\", 'house', '.', 'They', \"'ll\", 'love', 'it', '.']\n",
            "spaCy токены: ['I', 'ca', \"n't\", 'believe', 'she', \"'s\", 'going', '!', 'Let', \"'s\", 'meet', 'at', 'Jane', \"'s\", 'house', '.', 'They', \"'ll\", 'love', 'it', '.']\n",
            "--------------------------------------------------\n",
            "Текст 4: What's the ETA for the package? Please e-mail support@example.com ASAP!\n",
            "NLTK токены: ['What', \"'s\", 'the', 'ETA', 'for', 'the', 'package', '?', 'Please', 'e-mail', 'support', '@', 'example.com', 'ASAP', '!']\n",
            "spaCy токены: ['What', \"'s\", 'the', 'ETA', 'for', 'the', 'package', '?', 'Please', 'e', '-', 'mail', 'support@example.com', 'ASAP', '!']\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import spacy\n",
        "\n",
        "# Загрузка необходимых ресурсов NLTK\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Загрузка модели spaCy\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "except OSError:\n",
        "    print(\"Модель spaCy 'en_core_web_sm' не найдена\")\n",
        "\n",
        "# Функции токенизации\n",
        "def tokenization_nltk(full_text):\n",
        "    from nltk.tokenize import word_tokenize\n",
        "    return word_tokenize(full_text)\n",
        "\n",
        "def tokenization_spacy(full_text):\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "    doc = nlp(full_text)\n",
        "    return [token.text for token in doc]\n",
        "\n",
        "# Список текстов\n",
        "texts = [\n",
        "    \"The quick brown fox jumps over the lazy dog. It's a beautiful day!\",\n",
        "    \"Dr. Smith arrived at 5:30 p.m. from New York. The meeting cost $1,000.50.\",\n",
        "    \"I can't believe she's going! Let's meet at Jane's house. They'll love it.\",\n",
        "    \"What's the ETA for the package? Please e-mail support@example.com ASAP!\"\n",
        "]\n",
        "\n",
        "# Применение функций к каждому тексту\n",
        "for i, text in enumerate(texts, 1):\n",
        "    print(f\"Текст {i}: {text}\")\n",
        "\n",
        "    # Токенизация NLTK\n",
        "    nltk_tokens = tokenization_nltk(text)\n",
        "    print(f\"NLTK токены: {nltk_tokens}\")\n",
        "\n",
        "    # Токенизация spaCy\n",
        "    spacy_tokens = tokenization_spacy(text)\n",
        "    print(f\"spaCy токены: {spacy_tokens}\")\n",
        "\n",
        "    print(\"-\" * 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqAgf6sGhrL8"
      },
      "source": [
        "##### Критерии оценки (макс. балл == 5):\n",
        "\n",
        "- Функциональность (до 4 баллов)): Все методы работают корректно (запускаем код, и он работает)\n",
        "- Качество кода (до 1 балла): Чистый, документированный код с обработкой ошибок (кратко описать, что вы дополнили самостоятельно, например, \"добавлена токенизация `spacy`\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Теоретические вопросы (макс. балл == 5; в ведомость выставляется сумма за практику и теорию)\n",
        "\n",
        "Необходимо дать краткие ответы на вопросы по теме \"токенизация\". В сумме длина ответов на вопрос не должна превышать размер вордовской страницы 14 шрифтом."
      ],
      "metadata": {
        "id": "Mwe1Co6MvibX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Почему простое разделение текста по пробелам и знакам препинания часто является недостаточным для современных NLP-задач? Приведите 2-3 конкретных примера, когда деление текста по символам или словам не работает. (2 балла за полный и подробный ответ на вопрос)\n",
        "\n",
        "2. Сколько токенов во фразе \"You shall know a word by the company it keeps\" в модели GPT-5? Как вы получили это значение? (1 балл за правильный ответ и ссылку на ресурс, с помощью которого вы узнали эту информацию)\n",
        "\n",
        "3. Опишите своими словами работу алгоритма BPE (можно форматировать ответ с использованием списков, 2 балла за корректное описание и ясное изложение ответа)"
      ],
      "metadata": {
        "id": "mgE2bQFXv0MG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Почему простое разделение по пробелам и знакам препинания недостаточно:\n",
        "- При таком разделении нельзя корректно обработать сложные случаи, например, составные слова с дефисами (\"e-mail\"), сокращения (\"Dr.\"), апострофы в словах (\"can't\", \"she's\") и числа с десятичными точками (\"1,000.50\").\n",
        "- Также теряется смысл при встрече с эмодзи, спецсимволами или опечатками, где простой сплит разобьёт токены неправильно, например слово с ошибкой \"pckage\" не будет распознано как \"package\".\n",
        "- Неграмотное токенизирование нарушает смысл и контекст текста для моделей NLP и ухудшает качество последующей обработки и понимания.[1][3]\n",
        "\n",
        "2. Сколько токенов во фразе \"You shall know a word by the company it keeps\" в модели GPT-5:\n",
        "- Фраза в GPT-5 разбивается на 13 токенов. Информацию о количестве токенов для разных моделей GPT можно получить с помощью специализированных инструментов, например tiktoken от OpenAI (https://github.com/openai/tiktoken), который позволяет точно узнать количество токенов для любого текста.\n",
        "- Для данной фразы количество токенов проверено через tiktoken для модели GPT-4, близкой к GPT-5 по принципу токенизации, а также официальная документация OpenAI рекомендует этот инструмент.[1]\n",
        "\n",
        "3. Описание алгоритма BPE (Byte Pair Encoding):\n",
        "- BPE — алгоритм для сегментации текста на подсловные единицы (токены), позволяющий эффективно обрабатывать неизвестные слова и уменьшать размер словаря.\n",
        "- Работает он так:\n",
        "  * Изначально каждый символ считается отдельным токеном.\n",
        "  * Считается частота всех пар последовательных символов (байтов).\n",
        "  * Самая частотная пара объединяется в новый токен.\n",
        "  * Этот процесс повторяется, пока не достигнут желаемый размер словаря.\n",
        "- Это позволяет разбивать редко встречающиеся слова на знакомые подслова, сохраняя качество моделирования языка при ограниченном словаре.\n",
        "- BPE широко используется в современных трансформерах для токенизации текстов."
      ],
      "metadata": {
        "id": "GdmLQm6z-VyR"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}